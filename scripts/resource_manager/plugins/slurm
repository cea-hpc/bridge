#*****************************************************************************\
#  scripts/resource_manager/plugins/slurm - 
#******************************************************************************
#  Copyright  CEA/DAM/DIF (2012)
#
#  This file is part of Bridge, an abstraction layer to ease batch system and
#  resource manager usage in heterogeneous HPC environments.
#
#  Bridge is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Bridge is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Bridge.  If not, see <http://www.gnu.org/licenses/>
#*****************************************************************************/

# fonction interne permettat l'obtention d'info sur slurm
# les infos fournies sont :
#
# la major release : ${slurm_major}
# la minor release : ${slurm_minor}
# la release       : ${slurm_release}
#
# le parametre a utiliser pour la memoire : ${slurm_mem_param}
#
function _load_slurm_infos {

    ## on recupere les infos sur la version de slurm utilisee
    ## de la forme "Slurm x.y.z"
    slurm_infos=$(sinfo -V 2>/dev/null)
    slurm_major=${slurm_infos##* }
    slurm_major=${slurm_major%%.*}
    slurm_minor=${slurm_infos%.*}
    slurm_minor=${slurm_minor##*.}
    slurm_release=${slurm_infos##*.}

    ## on fixe le parametre pour la memoire
    if [[ ${slurm_major} -gt 1 ]]
	then
	slurm_mem_param="--mem-per-cpu"
    elif [[ ${slurm_major} -eq 1 ]] && [[ ${slurm_minor} -ge 4 ]]
	then
	slurm_mem_param="--mem-per-cpu"
    elif [[ ${slurm_major} -eq 1 ]] && [[ ${slurm_minor} -eq 3 ]] && [[ ${slurm_release} -ge 6 ]]
	then
	slurm_mem_param="--mem-per-cpu"
    else
	slurm_mem_param="--task-mem"
    fi

}

function spmd_allocate_cmd {

    ## on se place en mode debug si souhaite
    typeset -i BRIDGE_DEBUG_LEVEL=${BRIDGE_DEBUG_LEVEL:-0}+1
    if [ "${BRIDGE_DEBUG_MODE}" = "yes" ] && [[ ${BRIDGE_DEBUG_LEVEL} -le ${BRIDGE_DEBUG_MAXLEVEL} ]]
	then
	set -x
    fi

    ## on incremente le niveau de log
    typeset -i BRIDGE_VERBOSE_LEVEL=${BRIDGE_VERBOSE_LEVEL:-0}+1

    ## on indique le profile associe au plugin
    export BRIDGE_MPRUN_PROFILE=${BRIDGE_PROFILE_DIR}/slurm.sh

    ## on ajoute une entete a la log
    typeset LOGGER="${LOGGER} allocate: "

    typeset VERSION="slurm-1.0"
    typeset exit_status=0

    spmd_allocate_cmd=${spmd_allocate_cmd:-salloc}
    spmd_allocate_options=${spmd_allocate_options:--K}
    spmd_allocate_quiet=
    spmd_allocate_args=
    spmd_allocate_separator=${spmd_allocate_separator:---}

    spmd_allocate_ncore=
    spmd_allocate_nproc=
    spmd_allocate_nnode=
    spmd_allocate_partition=
    spmd_allocate_pproject=
    if [[ -z "$spmd_allocate_extra_parameters[@]" ]]; then
    typeset -a spmd_allocate_extra_parameters
    fi
    spmd_allocate_maxmem=
    spmd_allocate_maxtime=
    spmd_allocate_requestname=

    spmd_allocate_exclusive=no

    while getopts Vc:n:N:p:A:M:T:E:T:er: option
      do
      case ${option} in
	  
	  V)
	      echo ${VERSION}
	      return 0
	      ;;

	  c)
	      spmd_allocate_ncore=${OPTARG}
	      ;;

	  n)
	      spmd_allocate_nproc=${OPTARG}
	      ;;

	  N)
	      spmd_allocate_nnode=${OPTARG}
	      ;;

	  p)
	      spmd_allocate_partition=${OPTARG}
	      ;;

	  A)
	      spmd_allocate_project=${OPTARG}
	      ;;

	  E)
	      if [[ -n ${OPTARG} ]]
		  then
		  spmd_allocate_extra_parameters+=( "${OPTARG[@]}" )
	      fi
	      ;;

	  M)
	      spmd_allocate_maxmem=${OPTARG}
	      ;;

	  T)
	      spmd_allocate_maxtime=${OPTARG}
	      ;;

	  e)
	      spmd_allocate_exclusive=yes
	      ;;

	  r)
	      spmd_allocate_requestname=${OPTARG}
	      ;;

	  *)
	      exit_status=255
	      break
	      ;;
      esac
    done
    shift ${OPTIND}-1 2>/dev/null
    spmd_allocate_args=( "$@" )

    # on log un message si on est au sein d'une allocation
    if [[ ${BRIDGE_MPRUN_ALLOCATION} == yes ]]
	then
        ${LOGGER} "wrapping allocation detected, skipping this one"
        "${spmd_allocate_args[@]}"
        exit_status=$?
        ${LOGGER} "exiting with status ${exit_status}"
        return ${exit_status}
    fi

    ## on recupere les infos sur la version de slurm utilisee
    _load_slurm_infos

    # on appelle les addons eventuels en phase spmd_pre_allocate
    bridge_addons_call_function bridge_addon_mprun_spmd_pre_allocate "${spmd_run_args[@]}"
    
    ## si le code de retour de la stack des addons est 0
    ## alors le job n'a pas ete lance par un des addons
    ## donc on le lance, sinon, un des addons s'en est charge
    ## on passe au post run
    if [ $? -eq 0 ]
	then

	# build options
	if [[ ${spmd_allocate_exclusive} == yes ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} --exclusive"
	fi
	if [[ -n ${spmd_allocate_ncore} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} -c ${spmd_allocate_ncore}"
	fi
	if [[ -n ${spmd_allocate_nproc} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} -n ${spmd_allocate_nproc}"
	fi
	if [[ -n ${spmd_allocate_nnode} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} -N ${spmd_allocate_nnode}-${spmd_allocate_nnode}"
	fi
	if [[ -n ${spmd_allocate_partition} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} -p ${spmd_allocate_partition}"
	fi
	if [[ -n ${spmd_allocate_project} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} -A ${spmd_allocate_project}"
	fi
	if [[ -n ${spmd_allocate_maxmem} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} ${slurm_mem_param:---task-mem}=${spmd_allocate_maxmem}"
	fi
	typeset -i mintime
	typeset -i maxtime
	if [[ -n ${spmd_allocate_maxtime} ]]
	    then
	    mintime=${spmd_allocate_maxtime%%-*}
	    maxtime=${spmd_allocate_maxtime##*-}
	fi
	if [[ -n ${mintime} ]] && [[ -n ${maxtime} ]]
	then
	    typeset runlimit
	    typeset residu
	    if [[ ${mintime} -lt ${maxtime} ]]
	    then
		(( runlimit = mintime / 60 ))
		(( residu = mintime % 60 ))
		spmd_allocate_options="${spmd_allocate_options} --time-min ${runlimit}:${residu}"
	    fi
	    (( runlimit = maxtime / 60 ))
	    (( residu = maxtime % 60 ))
		spmd_allocate_options="${spmd_allocate_options} --time ${runlimit}:${residu}"
	fi
	if [[ -n ${spmd_allocate_requestname} ]]
	    then
	    spmd_allocate_options="${spmd_allocate_options} -J ${spmd_allocate_requestname}"
	fi

	## Check if we can use the quiet mode of SLURM
	echo "${spmd_allocate_extra_parameters[@]}" | egrep -qw '\-v+'
	if [[ $? -ne 0 ]]
	then
	    spmd_allocate_quiet="-Q "
	fi

        ## on export quelques variables d'environnement dont une indiquant que l'on 
	## a deja fait une allocation afin d'eviter les boucles
	export BRIDGE_MPRUN_ALLOCATION=yes
	${LOGGER} "${spmd_allocate_cmd} ${spmd_allocate_quiet}${spmd_allocate_options} ${spmd_allocate_extra_parameters[@]} ${spmd_allocate_separator} ${spmd_allocate_args[@]}"
	${spmd_allocate_cmd} ${spmd_allocate_quiet}${spmd_allocate_options} "${spmd_allocate_extra_parameters[@]}" ${spmd_allocate_separator} "${spmd_allocate_args[@]}"
	exit_status=$?
    fi

    # on appelle les addons eventuels en phase spmd_post_allocate
    bridge_addons_call_function bridge_addon_mprun_spmd_post_allocate "${spmd_allocate_args[@]}"

    ${LOGGER} "exiting with status ${exit_status}"
    return ${exit_status}
}



function spmd_run_cmd {

    ## on se place en mode debug si souhaite
    typeset -i BRIDGE_DEBUG_LEVEL=${BRIDGE_DEBUG_LEVEL:-0}+1
    if [ "${BRIDGE_DEBUG_MODE}" = "yes" ] && [[ ${BRIDGE_DEBUG_LEVEL} -le ${BRIDGE_DEBUG_MAXLEVEL} ]]
	then
	set -x
    fi

    ## on incremente le niveau de log
    typeset -i BRIDGE_VERBOSE_LEVEL=${BRIDGE_VERBOSE_LEVEL:-0}+1

    ## on indique le profile associe au plugin
    export BRIDGE_MPRUN_PROFILE=${BRIDGE_PROFILE_DIR}/slurm.sh

    ## on ajoute une entete a la log
    typeset LOGGER="${LOGGER} run: "

    typeset VERSION="slurm-1.0"
    typeset exit_status=0

    spmd_run_ncore=
    spmd_run_nproc=
    spmd_run_nnode=
    spmd_run_partition=
    spmd_run_project=
    if [[ -z "$spmd_run_extra_parameters[@]" ]]; then
    typeset -a spmd_run_extra_parameters
    fi
    spmd_run_maxmem=
    spmd_run_maxtime=
    spmd_run_separator=${spmd_run_separator:---}

    spmd_run_cmd=${spmd_run_cmd:-srun}
    spmd_run_options=${spmd_run_options}
    spmd_run_args=
    

    while getopts Vc:n:N:p:A:M:T:E: option
      do
      case ${option} in
	  
	  V)
	      echo ${VERSION}
	      return 0
	      ;;

	  c)
	      spmd_run_ncore=${OPTARG}
	      ;;

	  n)
	      spmd_run_nproc=${OPTARG}
	      ;;

	  N)
	      spmd_run_nnode=${OPTARG}
	      ;;

	  p)
	      spmd_run_partition=${OPTARG}
	      ;;

	  A)
	      spmd_run_project=${OPTARG}
	      ;;

	  E)
	      if [[ -n ${OPTARG} ]]
		  then
		  spmd_run_extra_parameters+=( "${OPTARG[@]}" )
	      fi
	      ;;

	  M)
	      spmd_run_maxmem=${OPTARG}
	      ;;

	  T)
	      spmd_run_maxtime=${OPTARG}
	      ;;

	  *)
	      exit_status=255
	      break
	      ;;
      esac
    done
    shift ${OPTIND}-1 2>/dev/null
    spmd_run_args=( "$@" )

    # on log un message si on est au sein d'une allocation
    if [[ ${BRIDGE_MPRUN_ALLOCATION} == yes ]]
	then
	${LOGGER} "wrapping allocation detected"
    fi

    ## on recupere les infos sur la version de slurm utilisee
    _load_slurm_infos
	
    # on appelle les addons eventuels en phase spmd_pre_run
    bridge_addons_call_function bridge_addon_mprun_spmd_pre_run $@

    ## si le code de retour de la stack des addons est 0
    ## alors le job n'a pas ete lance par un des addons
    ## donc on le lance, sinon, un des addons s'en est charge
    ## on passe au post run
    if [ $? -eq 0 ]
	then

        # build options if required (still srun after addons stages)
	# (or spmd_run_options is now "srun -a" meanings that a debug session like
	# totalview will take over)
	if [[ ${spmd_run_cmd} == "srun" ]] || [[ ${spmd_run_options} == "srun -a" ]]
	   then

	   if [[ -n ${spmd_run_ncore} ]]
	       then
	       spmd_run_options="${spmd_run_options} -c ${spmd_run_ncore}"
	   fi
	   if [[ -n ${spmd_run_nproc} ]]
	       then
	       spmd_run_options="${spmd_run_options} -n ${spmd_run_nproc}"
	   fi
	   if [[ -n ${spmd_run_nnode} ]]
	       then
	       spmd_run_options="${spmd_run_options} -N ${spmd_run_nnode}-${spmd_run_nnode}"
	   fi
	   if [[ -n ${spmd_run_partition} ]]
	       then
	       spmd_run_options="${spmd_run_options} -p ${spmd_run_partition}"
	   fi
	   if [[ -n ${spmd_run_project} ]]
	       then
	       spmd_run_options="${spmd_run_options} -A ${spmd_run_project}"
	   fi
	   if [[ -n ${spmd_run_maxmem} ]]
	       then
	       spmd_run_options="${spmd_run_options} ${slurm_mem_param:---task-mem}=${spmd_run_maxmem}"
	   fi
	   typeset -i mintime
	   typeset -i maxtime
	   if [[ -n ${spmd_run_maxtime} ]]
	   then
	       mintime=${spmd_run_maxtime%%-*}
	       maxtime=${spmd_run_maxtime##*-}
	   fi
	   if [[ -n ${mintime} ]] && [[ -n ${maxtime} ]]
	   then
	       typeset runlimit
	       typeset residu
	       if [[ ${mintime} -lt ${maxtime} ]]
	       then
		   (( runlimit = mintime / 60 ))
		   (( residu = mintime % 60 ))
		   spmd_run_options="${spmd_run_options} --time-min ${runlimit}:${residu}"
	       fi
	       (( runlimit = maxtime / 60 ))
	       (( residu = maxtime % 60 ))
	       spmd_run_options="${spmd_run_options} --time ${runlimit}:${residu}"
	   fi
	elif [[ ${spmd_run_cmd} == "mpirun" ]]
	    then
            if [[ "$BRIDGE_MPRUN_CHECK" =  "yes" ]] && [[ "$BRIDGE_MPRUN_ALLOCATION" = "yes" ]]  ;then
                SLURM_REQ=
                if [ -n "${spmd_run_ncore}" ] ; then
                   SLURM_REQ="$SLURM_REQ -c ${spmd_run_ncore}"
                fi
                if [[ -n ${spmd_run_nproc} ]] ; then
                   spmd_run_options="${spmd_run_options} -np ${spmd_run_nproc}"
                   SLURM_REQ="$SLURM_REQ -n ${spmd_run_nproc}"
                elif [[ -n ${SLURM_NPROCS} ]] ; then
                   spmd_run_options="${spmd_run_options} -np ${SLURM_NPROCS}"
                   if [ "$BRIDGE_CPUS_BINDING" != "no" ] ; then
			SLURM_REQ="$SLURM_REQ -n ${SLURM_NPROCS}"
                   fi
                elif [[ -n ${SLURM_NNODES} ]] ;  then
                ### when slurm allocated procs is 1 SLURM_NPROCS is not set
                ### we check that we are in slurm env and put 1 in -np in that
                ### case
                   spmd_run_options="${spmd_run_options} -np 1"
                fi
                if [ -n "${spmd_run_nnode}" ] ;then
                   SLURM_REQ="$SLURM_REQ  -N ${spmd_run_nnode}"
                fi
                if [ "$SLURM_REQ" != "" ] ; then
                   spmd_run_options="${spmd_run_options} --mca rmaps_base_cpus_per_proc 1 "
                   unset LS_COLORS
                   SLURM_VARS="$(srun $SLURM_REQ env | egrep  'SLURM_NPROCS|SLURM_TASKS_PER_NODE|SLURM_CPUS_PER_TASK' | sort | uniq | sed 's/(/\\(/g' | sed 's/)/\\)/g')"
                   if [ "$SLURM_VARS" != "" ] ; then
                      spmd_run_cmd="eval $SLURM_VARS BRIDGE_MPRUN_NCORE=\$SLURM_CPUS_PER_TASK mpirun --bind-to-none" 
		   # on fait une pause car le cpuset cree par le srun precedent est en cours
                   # de suppression sur les noeuds d'execution
		   # si on lance le deuxieme job trop vite, le cpuset ne sera pas recree car existant deja
		   # mais n'existera plus au moment d'y ajouter les taches d'ou probleme...
                      sleep 5
                   else
                      echo "Warning : cannot satisfy specified resources request inside current allocation" >&2
                   fi
                fi

            else
                 
	    ## openmpi seems to use a bad SLURM env value to guess the number
	    ## of processes to start, we force the np value to avoid problems
	    ## we use the input parameter or SLURM_NPROCS if defined and not
	    ## the first one
	    if [[ -n ${spmd_run_nproc} ]]
		then
		spmd_run_options="${spmd_run_options} -np ${spmd_run_nproc}"
	    elif [[ -n ${SLURM_NPROCS} ]]
		then
		spmd_run_options="${spmd_run_options} -np ${SLURM_NPROCS}"
	    elif [[ -n ${SLURM_NNODES} ]]
		then
		### when slurm allocated procs is 1 SLURM_NPROCS is not set
		### we check that we are in slurm env and put 1 in -np in that
		### case
		spmd_run_options="${spmd_run_options} -np 1"
	    fi
          fi
	fi
	
	## as we remove the eval in front of the execution, we can no longer use spmd_run_cmd="VARNAME=value ${spmd_run_cmd}"
	## to modify the behavior of spmd_run_cmd. As a result, we have to have different execution paths...
	if [[ -n ${spmd_run_ncore} ]] && [[ ${spmd_run_cmd} == "mpirun" ]]
        then
	    ${LOGGER} $(echo "SLURM_CPUS_PER_TASK=${spmd_run_ncore} ${spmd_run_cmd} ${spmd_run_options} ${spmd_run_extra_parameters[@]} ${spmd_run_separator} ${spmd_run_args[@]}")
	    SLURM_CPUS_PER_TASK=${spmd_run_ncore} ${spmd_run_cmd} ${spmd_run_options} "${spmd_run_extra_parameters[@]}" ${spmd_run_separator} "${spmd_run_args[@]}"
	else
	    ${LOGGER} $(echo "${spmd_run_cmd} ${spmd_run_options} ${spmd_run_extra_parameters[@]} ${spmd_run_separator} ${spmd_run_args[@]}")
	    ${spmd_run_cmd} ${spmd_run_options} "${spmd_run_extra_parameters[@]}" ${spmd_run_separator} "${spmd_run_args[@]}"
        fi

	exit_status=$?
    fi

    # on appelle les addons eventuels en phase spmd_post_run
    bridge_addons_call_function bridge_addon_mprun_spmd_post_run "${spmd_run_args[@]}"
    
    ${LOGGER} "exiting with status ${exit_status}"
    return ${exit_status}
}

function mpinfo_print_status {

    typeset cmd="sinfo"
    partition_arg=
    if [[ -n $1 ]]
    then
        partition_arg=$1
        cmd="${cmd} -p $1"
    fi

if [  -z "$partition_arg"  ] ; then
      echo "                      --------------CPUS------------  -------------NODES------------"
      echo "PARTITION    STATUS   TOTAL   DOWN    USED    FREE    TOTAL   DOWN    USED    FREE     MpC    CpN  SpN CpS  TpC GpN GPU Type"
      echo "---------    ------   ------  ------  ------  ------  ------  ------  ------  ------   ------ ---- --- ---- --- --- --------"
fi

${cmd} -eo  "%R %a %C %F %m %X %Y %Z %c %G %N" -h | sort -nr -k 4 -t'/' | while read PARTITION STATE CPUS_STATE NODES_STATE MEMPERNODE SOCKETSPERNODE CORESPERSOCKET THREADSPERCORE CPUSPERNODE GRES NODES
do
    set $(echo $CPUS_STATE $NODES_STATE | tr -s "/" " ")
    MEMPERCORE=
    if [ $CPUSPERNODE != 0 ]; then
        (( MEMPERCORE = MEMPERNODE / CPUSPERNODE ))
    fi
    GPUSPERNODE=
    GPUSTYPE=
    for TAG in $(echo $GRES | tr -s "," " ")
    do
       if [ "$(echo $TAG | grep gpu)" != "" ] ;then
           GPUDATA=$(echo $TAG | awk -F '(' '{print $1}')
           GPUSTYPE=$(echo $GPUDATA | cut -d":" -f2)
           GPUSPERNODE=$(echo $GPUDATA | cut -d":" -f3)
           break
       fi
    done
    GPUSPERNODE=${GPUSPERNODE:=0}
    if [  -z "$partition_arg" ] ; then
          printf "%-12s %-8s %6d  %6d  %6d  %6d  %6d  %6d  %6d  %6d   %6d  %3d  %2d  %3d  %2d  %2d  %s\n" $PARTITION $STATE  $4 $3  $1 $2 $8 $7 $5 $6 $MEMPERCORE $CPUSPERNODE $SOCKETSPERNODE $CORESPERSOCKET $THREADSPERCORE $GPUSPERNODE $GPUSTYPE
    else
          RESERVED_NODES=$(sinfo -t reserved -p  $PARTITION  | tail -1 | awk '{if ($4 != 0) {print $4, $6}}')
          printf "\
partition           : $PARTITION
status              : $STATE
number of nodes     : $8
number of cores     : $4
memory per node     : $MEMPERNODE Mo
max mem per core    : $MEMPERCORE Mo
cpus per node       : $CPUSPERNODE
sockets per node    : $SOCKETSPERNODE
cores per socket    : $CORESPERSOCKET
threads per core    : $THREADSPERCORE
gpus per node       : $GPUSPERNODE
nodes list          : $NODES
reserved nodes      : $(echo $RESERVED_NODES | cut -d' ' -f1)
reserved nodes list : $(echo $RESERVED_NODES | cut -d' ' -f2)\n"
    fi
done

}


function mpinfo_print_limits {

     typeset cmd="sinfo"
     partition_arg=
     if [[ -n $1 ]]
     then
         partition_arg=$1
         cmd="${cmd} -p $1"
     fi

     if [  -z "$partition_arg"  ] ; then
       echo "PARTITION    STATUS   GROUPS  TIMELIMIT  JOBSIZE"
       echo "---------    ------   ------  ---------  -------"
     fi
     ${cmd} -eo "%12P %8a %7g %10l %s" -h
}

# function mpinfo_printinfo {
# echo "                      ----------CPUS---------"
# echo "PARTITION    STATUS   TOTAL  DOWN  USED  FREE  MEM  NODES"
# echo "---------    ------   -----  ----  ----  ----  ---  -----"
# sinfo -o  "%P %a %C %m %N" -h | sort | while read PARTITION STATUS CPUS_STATE MEMORY NODES
# do
#         set $(echo $CPUS_STATE | tr -s "/" " ")
#         printf "%-12s %-8s  %4d  %4d  %4d  %4d %4d  %s\n" $PARTITION $STATUS  $4 $3  $1 $2 $MEMORY $NODES
# done
# }


function mpinfo_cmd {

    ## on se place en mode debug si souhaite
    typeset -i BRIDGE_DEBUG_LEVEL=${BRIDGE_DEBUG_LEVEL:-0}+1
    if [ "${BRIDGE_DEBUG_MODE}" = "yes" ] && [[ ${BRIDGE_DEBUG_LEVEL} -le ${BRIDGE_DEBUG_MAXLEVEL} ]]
	then
	set -x
    fi

    ## on incremente le niveau de log
    typeset -i BRIDGE_VERBOSE_LEVEL=${BRIDGE_VERBOSE_LEVEL:-0}+1

    ## on ajoute une entete a la log
    typeset LOGGER="${LOGGER} mpinfo: "

    typeset VERSION="slurm-1.0"
    typeset exit_status=0

    typeset partition_name=
    typeset requested_info=status
    typeset cmd=mpinfo_print_status


    while getopts Vcp:l option
      do
      case ${option} in
	  
	  V)
	      echo ${VERSION}
	      return 0
	      ;;
          l)
              requested_info=limits
              cmd=mpinfo_print_limits
              ;;


	  p)
	      partition_name=${OPTARG}
	      ;;

	  *)
	      exit_status=255
	      break
	      ;;
      esac
    done
    shift ${OPTIND}-1 2>/dev/null

    ## on recupere les infos sur la version de slurm utilisee
    _load_slurm_infos

    eval ${cmd} ${partition_name}

    exit_status=$?

    ${LOGGER} "exiting with status ${exit_status}"
    return ${exit_status}
}

function mpstat_cmd {

    ## on se place en mode debug si souhaite
    typeset -i BRIDGE_DEBUG_LEVEL=${BRIDGE_DEBUG_LEVEL:-0}+1
    if [ "${BRIDGE_DEBUG_MODE}" = "yes" ] && [[ ${BRIDGE_DEBUG_LEVEL} -le ${BRIDGE_DEBUG_MAXLEVEL} ]]
        then
        set -x
    fi

    ## on incremente le niveau de log
    typeset -i BRIDGE_VERBOSE_LEVEL=${BRIDGE_VERBOSE_LEVEL:-0}+1

    ## on ajoute une entete a la log
    typeset LOGGER="${LOGGER} mpstat: "

    typeset VERSION="slurm-1.0"
    typeset exit_status=0
    while getopts Vr:t:n:m:a: option
      do
      case ${option} in

          V)
              echo ${VERSION}
              return 0
              ;;

          r)
    	     r_flag=yes
             resid=${OPTARG}
              ;;

          a)
             a_flag=yes
             resid=${OPTARG}
             ;;
                            

          t)
             t_flag=yes
             fullid=${OPTARG}
              ;;

          n)
             n_flag=yes
             nodeid=${OPTARG}
              ;;

          m)
             m_flag=yes
             resid=${OPTARG}
              ;;

          *)
              exit_status=255
              break
              ;;
      esac
    done
    shift ${OPTIND}-1 2>/dev/null

    ## on recupere les infos sur la version de slurm utilisee
    _load_slurm_infos

    if  [[ ${r_flag} = yes ]]
	then 
	#stat_cmd="squeue -o '%i %.8u %.9P %.8j %.2t %.8r %.9M %.9l %.6D %.5C %.7m' -j ${resid}" 
	stat_cmd="scontrol -d show job ${resid}"

    elif  [[ ${a_flag} = yes ]]
        then
        stat_cmd="squeue -j ${resid} --steps -o '%10i %.10j %5A %.10P %20S %.9M' "

    elif  [[ ${m_flag} = yes ]]
    then
        scontrol -d show job ${resid} |  while read LINE
        do
         case "$LINE" in
          *Task=*  ) set $LINE ; CPUSPERTASK=$(echo $3 | cut -d"=" -f2) ;;
          *CPU_IDs=* ) eval $LINE ; NODES=${Nodes} ; NBCPUS=$(nodeset -c a[${CPU_IDs}]) ; MEM=${Mem}
          typeset -i NBTASKS=$NBCPUS/$CPUSPERTASK
          echo
          tput smso ; echo   "Node(s)=$NODES Nb_tasks=$NBTASKS Nb_cores=$NBCPUS CPU_ids=${CPU_IDs} MaxRSS=$MEM Mo" ; tput rmso
          clush --nostdin -w $NODES -b  ${BRIDGE_BASEDIR}/share/scripts/resource_manager/addons/get_task_info  ${resid}
          ;;
         esac
       done
       stat_cmd="true"
       

    elif  [[ ${t_flag} = yes ]]
    then
        stat_cmd="clustack slurmjob:$resid"
    elif  [[ ${n_flag} = yes ]]
    then  
#	stat_cmd="RESID_LIST=\$(${BRIDGE_RMASTAT_CMD} -n ${nodeid} -o id) ; ${BRIDGE_MPSTAT_CMD} -r \$(echo \$RESID_LIST | tr -s ' ' ',')"
#	stat_cmd="RESID_LIST=\$(ssh ${nodeid} ps -o cmd -e | grep '^slurmstepd' | cut -d'[' -f2 | cut -d'.' -f1) ; ${BRIDGE_MPSTAT_CMD} -r \$(echo \$RESID_LIST | tr -s ' ' ',')"
	stat_cmd="${BRIDGE_RMASTAT_CMD} -n ${nodeid} -c"
    fi

    eval ${stat_cmd}
    exit_status=$?

    ${LOGGER} "exiting with status ${exit_status}"
    return ${exit_status}
}
